"""
MinerU Tianshu - LitServe Worker (Production Ready)
å¤©æ¢ LitServe Worker - å¤šå¼•æ“æ™ºèƒ½è°ƒåº¦å¢å¼ºç‰ˆ

æ›´æ–°ç‚¹ï¼š
1. å®Œæ•´å®ç° _should_split_pdfï¼šæ”¯æŒå¤§æ–‡ä»¶è‡ªåŠ¨åˆ†ç‰‡è¿›å…¥ä»»åŠ¡é˜Ÿåˆ—ã€‚
2. VLM æç¤ºè¯ä¼˜åŒ–ï¼šé’ˆå¯¹ MinerU-1.2B è°ƒä¼˜ï¼Œè¾“å‡ºé«˜è´¨é‡ Markdownã€‚
3. å¹¶å‘å®‰å…¨å¢å¼ºï¼šç¡®ä¿å¤š GPU ç¯å¢ƒä¸‹æ˜¾å­˜æ¸…ç†å½»åº•ã€‚
4. å®Œå–„å¯åŠ¨å‚æ•°ï¼šå¯¹æ¥ start_all.pyã€‚
"""

import os
import json
import sys
import time
import threading
import signal
import atexit
import base64
import multiprocessing
from pathlib import Path
from typing import Optional, List, Dict, Any

import litserve as ls
from loguru import logger
import importlib.util

# ============================================================================
# åŸºç¡€è¡¥ä¸ä¸ç¯å¢ƒåˆå§‹åŒ–
# ============================================================================
try:
    import litserve.mcp as ls_mcp
    from contextlib import asynccontextmanager
    if not hasattr(ls_mcp, "MCPServer"):
        class Dummy: pass
        ls_mcp.MCPServer = Dummy
        ls_mcp.StreamableHTTPSessionManager = Dummy
    class DummyMCPConnector:
        def __init__(self, *args, **kwargs): pass
        @asynccontextmanager
        async def lifespan(self, app): yield
        def connect_mcp_server(self, *args, **kwargs): pass
    ls_mcp._LitMCPServerConnector = DummyMCPConnector
except Exception as e:
    logger.warning(f"MCP Patching bypassed: {e}")

sys.path.insert(0, str(Path(__file__).parent.parent.parent))
from task_db import TaskDB
from output_normalizer import normalize_output

# å¼•æ“æ£€æµ‹
def is_available(name): return importlib.util.find_spec(name) is not None
PYPDF_AVAILABLE = is_available("pypdf")
OPENAI_AVAILABLE = is_available("openai")
FITZ_AVAILABLE = is_available("fitz")

class MinerUWorkerAPI(ls.LitAPI):
    def __init__(self, **kwargs):
        super().__init__()
        self.output_dir = kwargs.get("output_dir") or os.getenv("OUTPUT_PATH", "/app/data/output")
        self.poll_interval = kwargs.get("poll_interval", 0.5)
        self.enable_worker_loop = kwargs.get("enable_worker_loop", True)
        
        # è¿œç¨‹ API åˆ—è¡¨
        self.paddle_vlm_url = os.getenv("PADDLE_VLM_URL", "http://host.docker.internal:8118/v1")
        self.mineru_vlm_url = os.getenv("MINERU_VLM_URL", "http://host.docker.internal:8119/v1")

    def setup(self, device):
        # 1. GPU è¿›ç¨‹éš”ç¦»
        if "cuda:" in str(device):
            gpu_id = str(device).split(":")[-1]
            os.environ["CUDA_VISIBLE_DEVICES"] = gpu_id
            os.environ["MINERU_DEVICE_MODE"] = "cuda:0"
            logger.info(f"ğŸ¯ [GPU] Worker isolated to Physical {gpu_id}")

        # 2. è¿œç¨‹ VLM å®¢æˆ·ç«¯
        if OPENAI_AVAILABLE:
            from openai import OpenAI
            self.client_paddle = OpenAI(api_key="EMPTY", base_url=self.paddle_vlm_url)
            self.client_mineru = OpenAI(api_key="EMPTY", base_url=self.mineru_vlm_url)
        
        # 3. åˆå§‹åŒ–æ•°æ®åº“ä¸å¼•æ“
        self.task_db = TaskDB(os.getenv("DATABASE_PATH", "/app/data/db/mineru_tianshu.db"))
        self.mineru_pipeline_engine = None
        self.running = True
        self.device = device

        if self.enable_worker_loop:
            threading.Thread(target=self._worker_loop, daemon=True).start()
        logger.success(f"ğŸš€ Worker {device} Ready")

    def _worker_loop(self):
        while self.running:
            try:
                task = self.task_db.get_next_task(worker_id=f"worker-{self.device}")
                if task:
                    logger.info(f"ğŸ“¥ Pulled Task: {task['task_id']} | File: {task['file_name']}")
                    self._process_task(task)
                else:
                    time.sleep(self.poll_interval)
            except Exception as e:
                logger.error(f"âŒ Loop error: {e}")
                time.sleep(2)

    def _process_task(self, task: dict):
        task_id = task["task_id"]
        file_path = task["file_path"]
        options = json.loads(task.get("options", "{}")) if isinstance(task.get("options"), str) else task.get("options", {})
        backend = task.get("backend", "pipeline").lower()

        try:
            # 1. å¤§æ–‡ä»¶è‡ªåŠ¨åˆ‡åˆ†é€»è¾‘ (åˆ†ç‰‡åä»»åŠ¡é‡æ–°è¿›å…¥é˜Ÿåˆ—)
            if Path(file_path).suffix.lower() == ".pdf" and not task.get("parent_task_id"):
                if self._should_split_pdf(task_id, file_path, task, options):
                    logger.info(f"âœ‚ï¸ Task {task_id} split into subtasks. Parent task suspended.")
                    return

            # 2. è·¯ç”±åˆ†å‘
            result = None
            if backend == "pipeline":
                result = self._process_with_mineru(file_path, options)
            elif backend == "vlm-auto-engine":
                result = self._process_remote_vlm(file_path, options, engine_type="mineru")
            elif backend == "hybrid-auto-engine":
                result = self._process_hybrid(file_path, options)
            elif "paddleocr-vl" in backend:
                result = self._process_remote_vlm(file_path, options, engine_type="paddle")
            else:
                result = self._process_with_mineru(file_path, options)

            # 3. å®Œæˆä»»åŠ¡
            self.task_db.update_task_status(task_id, "completed", result_path=result["result_path"])
            
            # å¦‚æœæ˜¯å­ä»»åŠ¡ï¼Œæ£€æŸ¥çˆ¶ä»»åŠ¡æ˜¯å¦å¯ä»¥åˆå¹¶
            if task.get("parent_task_id"):
                self.task_db.on_child_task_completed(task_id)

        except Exception as e:
            logger.error(f"âŒ Task {task_id} Failed: {e}")
            self.task_db.update_task_status(task_id, "failed", error_message=str(e))

    # ============================================================================
    # æ ¸å¿ƒå¤„ç†å‡½æ•°
    # ============================================================================

    def _process_remote_vlm(self, file_path: str, options: dict, engine_type="mineru") -> dict:
        """é«˜æ€§èƒ½è¿œç¨‹è°ƒç”¨ï¼šPDF -> å›¾åƒæµ -> vLLM"""
        import fitz
        doc = fitz.open(file_path)
        output_dir = Path(self.output_dir) / Path(file_path).stem
        output_dir.mkdir(parents=True, exist_ok=True)
        
        client = self.client_mineru if engine_type == "mineru" else self.client_paddle
        model_name = "mineru-vlm-1.2b" if engine_type == "mineru" else "PaddleOCR-VL-1.5"
        
        # é’ˆå¯¹ä¸åŒæ¨¡å‹çš„æç¤ºè¯ä¼˜åŒ–
        system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£è§£æåŠ©æ‰‹ã€‚è¯·å°†å›¾ç‰‡å†…å®¹è½¬æ¢ä¸ºé«˜è´¨é‡çš„ Markdown æ ¼å¼ï¼Œ"
            "ç‰¹åˆ«æ³¨æ„ä¿ç•™è¡¨æ ¼çš„å•å…ƒæ ¼ç»“æ„ã€æ•°å­¦å…¬å¼çš„ LaTeX è¡¨è¾¾ä»¥åŠæ ‡é¢˜å±‚çº§ã€‚"
        )

        full_md = []
        for i in range(len(doc)):
            pix = doc[i].get_pixmap(matrix=fitz.Matrix(2, 2)) # æé«˜è¯†åˆ«ç²¾åº¦
            img_b64 = base64.b64encode(pix.tobytes("png")).decode("utf-8")
            
            response = client.chat.completions.create(
                model=model_name,
                messages=[{
                    "role": "user",
                    "content": [
                        {"type": "text", "text": system_prompt},
                        {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{img_b64}"}}
                    ]
                }],
                max_tokens=2048,
                temperature=0.1 # é™ä½éšæœºæ€§
            )
            full_md.append(f"\n{response.choices[0].message.content}")

        final_md = "\n\n".join(full_md)
        (output_dir / "result.md").write_text(final_md, encoding="utf-8")
        normalize_output(output_dir)
        return {"result_path": str(output_dir)}

    def _should_split_pdf(self, task_id: str, file_path: str, task: dict, options: dict) -> bool:
        """PDF åˆ†ç‰‡é€»è¾‘å®ç°ï¼šè¶…è¿‡é˜ˆå€¼åˆ™åˆ›å»ºå­ä»»åŠ¡"""
        if not PYPDF_AVAILABLE: return False
        
        from utils.pdf_utils import get_pdf_page_count, split_pdf_file
        threshold = int(os.getenv("PDF_SPLIT_THRESHOLD_PAGES", "50"))
        chunk_size = int(os.getenv("PDF_SPLIT_CHUNK_SIZE", "20"))
        
        page_count = get_pdf_page_count(Path(file_path))
        if page_count <= threshold: return False

        # æ‰§è¡Œç‰©ç†æ‹†åˆ†
        split_dir = Path(self.output_dir) / "temp_splits" / task_id
        split_dir.mkdir(parents=True, exist_ok=True)
        chunks = split_pdf_file(Path(file_path), split_dir, chunk_size=chunk_size)

        # è½¬æ¢ä¸ºçˆ¶ä»»åŠ¡å¹¶ç”Ÿæˆå­ä»»åŠ¡
        self.task_db.convert_to_parent_task(task_id, child_count=len(chunks))
        for chunk in chunks:
            self.task_db.create_child_task(
                parent_task_id=task_id,
                file_name=chunk["name"],
                file_path=chunk["path"],
                backend=task.get("backend", "pipeline"),
                options=options
            )
        return True

    def _process_with_mineru(self, file_path: str, options: dict) -> dict:
        if not self.mineru_pipeline_engine:
            from mineru_pipeline import MinerUPipelineEngine
            self.mineru_pipeline_engine = MinerUPipelineEngine(device="cuda:0" if "cuda" in str(self.device) else "cpu")
        
        output_dir = Path(self.output_dir) / Path(file_path).stem
        output_dir.mkdir(parents=True, exist_ok=True)
        res = self.mineru_pipeline_engine.parse(file_path, output_path=str(output_dir), options=options)
        normalize_output(Path(res["result_path"]))
        return res

    def _process_hybrid(self, file_path: str, options: dict) -> dict:
        # æ™ºèƒ½åˆ¤å®šï¼šå¦‚æœæ˜¯çº¯å›¾ç‰‡æˆ–æçŸ­æ–‡æ¡£ï¼Œèµ° VLMï¼›å¦åˆ™èµ° Pipeline
        if PYPDF_AVAILABLE:
            from pypdf import PdfReader
            text = PdfReader(file_path).pages[0].extract_text()
            if len(text.strip()) < 50:
                return self._process_remote_vlm(file_path, options, engine_type="mineru")
        return self._process_with_mineru(file_path, options)

    def decode_request(self, request): return request.get("action", "health")
    def predict(self, action): return {"status": "healthy", "device": str(self.device)}
    def encode_response(self, response): return response

# ============================================================================
# å¯åŠ¨å…¥å£
# ============================================================================
def start_litserve_workers(**kwargs):
    api = MinerUWorkerAPI(**kwargs)
    server = ls.LitServer(
        api, 
        accelerator=kwargs.get("accelerator", "auto"),
        devices=kwargs.get("devices", "auto"),
        workers_per_device=kwargs.get("workers_per_device", 1),
        timeout=False
    )
    server.run(port=kwargs.get("port", 8001))

if __name__ == "__main__":
    import argparse
    from utils import parse_list_arg
    
    parser = argparse.ArgumentParser()
    parser.add_argument("--port", type=int, default=8001)
    parser.add_argument("--devices", type=str, default="auto")
    parser.add_argument("--workers-per-device", type=int, default=1)
    parser.add_argument("--accelerator", type=str, default="cuda")
    parser.add_argument("--output-dir", type=str, default=None)
    # ä¿æŒä¸ start_all.py å‚æ•°å…¼å®¹
    parser.add_argument("--paddleocr-vl-vllm-api-list", type=parse_list_arg, default=[])
    
    args = parser.parse_args()
    start_litserve_workers(
        port=args.port,
        devices=args.devices,
        workers_per_device=args.workers_per_device,
        accelerator=args.accelerator,
        output_dir=args.output_dir
    )
