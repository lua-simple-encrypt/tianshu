"""
MinerU Tianshu - LitServe Worker (Production Ready)
å¤©æ¢ LitServe Worker - å¤šå¼•æ“æ™ºèƒ½è°ƒåº¦å¢å¼ºç‰ˆ

æ ¸å¿ƒé€»è¾‘ï¼š
- è°ƒåº¦ä¸­å¿ƒï¼šè´Ÿè´£æœ¬åœ° Pipelineã€è¿œç¨‹ VLMã€æ™ºèƒ½æ··åˆåŠ¨åŠ›çš„è·¯ç”±åˆ†å‘ã€‚
- å¹¶è¡ŒåŠ é€Ÿï¼šæ”¯æŒ PDF è‡ªåŠ¨åˆ†ç‰‡ï¼Œåˆ©ç”¨å¤š GPU èŠ‚ç‚¹å¹¶è¡Œå¤„ç†ã€‚
- ç»“æœé—­ç¯ï¼šå­ä»»åŠ¡å®Œæˆåè‡ªåŠ¨è§¦å‘ Markdown/JSON ç»“æœåˆå¹¶ã€‚
"""

import os
import json
import sys
import time
import threading
import signal
import atexit
import base64
import gc
import multiprocessing
from pathlib import Path
from typing import Optional, List, Dict, Any

import litserve as ls
from loguru import logger
import importlib.util

# ============================================================================
# åŸºç¡€è¡¥ä¸ä¸ç¯å¢ƒåˆå§‹åŒ–
# ============================================================================
try:
    import litserve.mcp as ls_mcp
    from contextlib import asynccontextmanager
    if not hasattr(ls_mcp, "MCPServer"):
        class Dummy: pass
        ls_mcp.MCPServer = Dummy
        ls_mcp.StreamableHTTPSessionManager = Dummy
    class DummyMCPConnector:
        def __init__(self, *args, **kwargs): pass
        @asynccontextmanager
        async def lifespan(self, app): yield
        def connect_mcp_server(self, *args, **kwargs): pass
    ls_mcp._LitMCPServerConnector = DummyMCPConnector
except Exception as e:
    logger.warning(f"MCP Patching bypassed: {e}")

# è®¾ç½®é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))
from task_db import TaskDB
from output_normalizer import normalize_output
from utils.merge_utils import merge_subtask_results # å¯¼å…¥åˆå¹¶å·¥å…·

# å¼•æ“å¯ç”¨æ€§æ£€æµ‹
def is_available(name): return importlib.util.find_spec(name) is not None
PYPDF_AVAILABLE = is_available("pypdf")
OPENAI_AVAILABLE = is_available("openai")
FITZ_AVAILABLE = is_available("fitz")

class MinerUWorkerAPI(ls.LitAPI):
    def __init__(self, **kwargs):
        super().__init__()
        self.output_dir = kwargs.get("output_dir") or os.getenv("OUTPUT_PATH", "/app/data/output")
        self.poll_interval = kwargs.get("poll_interval", 0.5)
        self.enable_worker_loop = kwargs.get("enable_worker_loop", True)
        
        # è¿œç¨‹ API é›†ç¾¤é…ç½®
        self.paddle_vlm_url = os.getenv("PADDLE_VLM_URL", "http://host.docker.internal:8118/v1")
        self.mineru_vlm_url = os.getenv("MINERU_VLM_URL", "http://host.docker.internal:8119/v1")

    def setup(self, device):
        # 1. ç‰©ç† GPU éš”ç¦» (å®ç°é€»è¾‘ï¼šPhysical ID -> Logical cuda:0)
        if "cuda:" in str(device):
            gpu_id = str(device).split(":")[-1]
            os.environ["CUDA_VISIBLE_DEVICES"] = gpu_id
            os.environ["MINERU_DEVICE_MODE"] = "cuda:0"
            logger.info(f"ğŸ¯ [GPU Isolation] Worker bound to Physical GPU {gpu_id}")

        # 2. åˆå§‹åŒ– OpenAI å…¼å®¹å®¢æˆ·ç«¯ï¼ˆå¯¹æ¥è¿œç¨‹ vLLMï¼‰
        if OPENAI_AVAILABLE:
            from openai import OpenAI
            self.client_paddle = OpenAI(api_key="EMPTY", base_url=self.paddle_vlm_url)
            self.client_mineru = OpenAI(api_key="EMPTY", base_url=self.mineru_vlm_url)
        
        # 3. åˆå§‹åŒ–æŒä¹…åŒ–å±‚
        self.task_db = TaskDB(os.getenv("DATABASE_PATH", "/app/data/db/mineru_tianshu.db"))
        self.mineru_pipeline_engine = None # å»¶è¿ŸåŠ è½½æœ¬åœ°æ¨¡å‹
        self.running = True
        self.device = device

        if self.enable_worker_loop:
            threading.Thread(target=self._worker_loop, daemon=True).start()
        logger.success(f"ğŸš€ Worker {device} Setup Complete")

    def _worker_loop(self):
        """Worker ä¸»åŠ¨æ‹‰å–ä»»åŠ¡å¾ªç¯"""
        while self.running:
            try:
                task = self.task_db.get_next_task(worker_id=f"worker-{self.device}")
                if task:
                    logger.info(f"ğŸ“¥ Pull Task: {task['task_id']} (Backend: {task.get('backend')})")
                    self._process_task(task)
                else:
                    time.sleep(self.poll_interval)
            except Exception as e:
                logger.error(f"âŒ Worker loop error: {e}")
                time.sleep(2)

    def _process_task(self, task: dict):
        """æ ¸å¿ƒå¤„ç†è·¯ç”±"""
        task_id = task["task_id"]
        file_path = task["file_path"]
        options = json.loads(task.get("options", "{}")) if isinstance(task.get("options"), str) else task.get("options", {})
        backend = task.get("backend", "pipeline").lower()

        try:
            # 1. æ£€æŸ¥æ˜¯å¦éœ€è¦è§¦å‘åˆ†ç‰‡é€»è¾‘
            if Path(file_path).suffix.lower() == ".pdf" and not task.get("parent_task_id"):
                if self._should_split_pdf(task_id, file_path, task, options):
                    return # ä»»åŠ¡å·²è£‚å˜ä¸ºå­ä»»åŠ¡ï¼Œå½“å‰æµç¨‹ä¸­æ­¢

            # 2. è·¯ç”±åˆ†å‘é€»è¾‘
            result = None
            if backend == "pipeline":
                result = self._process_with_mineru(file_path, options)
            elif backend == "vlm-auto-engine":
                result = self._process_remote_vlm(file_path, options, engine_type="mineru")
            elif backend == "hybrid-auto-engine":
                result = self._process_hybrid(file_path, options)
            elif "paddleocr-vl" in backend:
                result = self._process_remote_vlm(file_path, options, engine_type="paddle")
            else:
                logger.warning(f"âš ï¸ Unknown backend {backend}, routing to pipeline")
                result = self._process_with_mineru(file_path, options)

            # 3. æäº¤ä»»åŠ¡ç»“æœ
            self.task_db.update_task_status(task_id, "completed", result_path=result["result_path"])
            
            # 4. ã€æ ¸å¿ƒã€‘å¦‚æœæ˜¯å­ä»»åŠ¡ï¼Œæ£€æŸ¥å¹¶è§¦å‘çˆ¶çº§åˆå¹¶
            if task.get("parent_task_id"):
                parent_id = self.task_db.on_child_task_completed(task_id)
                if parent_id:
                    logger.info(f"ğŸ§± All subtasks done. Merging results for Parent: {parent_id}")
                    subtasks = self.task_db.get_child_tasks(parent_id)
                    merge_subtask_results(parent_id, subtasks, Path(self.output_dir))
                    self.task_db.update_task_status(parent_id, "completed", 
                                                   result_path=str(Path(self.output_dir) / parent_id))

        except Exception as e:
            logger.error(f"âŒ Task {task_id} Failed: {e}")
            self.task_db.update_task_status(task_id, "failed", error_message=str(e))
            # çº§è”æ ‡è®°çˆ¶ä»»åŠ¡å¤±è´¥
            if task.get("parent_task_id"):
                self.task_db.on_child_task_failed(task_id, str(e))
        finally:
            self._clean_memory()

    # ============================================================================
    # å¼•æ“å®ç°
    # ============================================================================

    def _process_remote_vlm(self, file_path: str, options: dict, engine_type="mineru") -> dict:
        """è¿œç¨‹ VLM é›†ç¾¤è°ƒåº¦é€»è¾‘"""
        import fitz
        doc = fitz.open(file_path)
        output_dir = Path(self.output_dir) / Path(file_path).stem
        output_dir.mkdir(parents=True, exist_ok=True)
        
        client = self.client_mineru if engine_type == "mineru" else self.client_paddle
        model_name = "mineru-vlm-1.2b" if engine_type == "mineru" else "PaddleOCR-VL-1.5"
        
        system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªé«˜ç²¾åº¦çš„ OCR ä¸“å®¶ã€‚è¯·å°†å›¾ç‰‡å†…å®¹è½¬æ¢ä¸ºç¬¦åˆ Markdown è§„èŒƒçš„æ–‡æœ¬ï¼Œ"
            "ä¿ç•™æ‰€æœ‰æ•°å­¦å…¬å¼ï¼ˆä½¿ç”¨ LaTeXï¼‰ã€è¡¨æ ¼ï¼ˆä½¿ç”¨ Markdown è¡¨æ ¼ï¼‰å’Œå®Œæ•´çš„æ’ç‰ˆå±‚çº§ã€‚"
        )

        full_md = []
        logger.info(f"ğŸ”® [VLM] Forwarding {len(doc)} pages to {model_name}...")

        for i in range(len(doc)):
            # æ¸²æŸ“ 144 DPI (2.0 zoom) å›¾ç‰‡ï¼Œä¿è¯ OCR æ¸…æ™°åº¦
            pix = doc[i].get_pixmap(matrix=fitz.Matrix(2, 2))
            img_b64 = base64.b64encode(pix.tobytes("png")).decode("utf-8")
            
            response = client.chat.completions.create(
                model=model_name,
                messages=[{
                    "role": "user",
                    "content": [
                        {"type": "text", "text": system_prompt},
                        {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{img_b64}"}}
                    ]
                }],
                max_tokens=2048,
                temperature=0.05 # æ¥è¿‘ç¡®å®šæ€§è¾“å‡º
            )
            full_md.append(f"\n{response.choices[0].message.content}")

        final_content = "\n\n".join(full_md)
        (output_dir / "result.md").write_text(final_content, encoding="utf-8")
        normalize_output(output_dir)
        return {"result_path": str(output_dir)}

    def _process_hybrid(self, file_path: str, options: dict) -> dict:
        """æ™ºèƒ½æ··åˆå†³ç­–é€»è¾‘"""
        is_complex = False
        if PYPDF_AVAILABLE:
            from pypdf import PdfReader
            text = PdfReader(file_path).pages[0].extract_text()
            # ç®€å•å¯å‘å¼ï¼šå¦‚æœé¦–é¡µæ–‡å­—æå°‘ï¼ˆ<50å­—ç¬¦ï¼‰ï¼Œåˆ¤å®šä¸ºæ‰«æä»¶/å›¾è¡¨ï¼Œèµ° VLM
            if len(text.strip()) < 50: is_complex = True
            
        if is_complex:
            logger.info("âš–ï¸ [Hybrid] Complex doc detected -> Routing to VLM.")
            return self._process_remote_vlm(file_path, options, engine_type="mineru")
        else:
            logger.info("âš–ï¸ [Hybrid] Standard doc detected -> Routing to Local Pipeline.")
            return self._process_with_mineru(file_path, options)

    def _should_split_pdf(self, task_id: str, file_path: str, task: dict, options: dict) -> bool:
        """é«˜æ€§èƒ½åˆ†ç‰‡é€»è¾‘"""
        if not PYPDF_AVAILABLE: return False
        from utils.pdf_utils import get_pdf_page_count, split_pdf_file
        
        # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½® (é»˜è®¤ 50 é¡µæ‹†åˆ†)
        threshold = int(os.getenv("PDF_SPLIT_THRESHOLD_PAGES", "50"))
        chunk_size = int(os.getenv("PDF_SPLIT_CHUNK_SIZE", "20"))
        
        pages = get_pdf_page_count(Path(file_path))
        if pages <= threshold: return False

        logger.info(f"âœ‚ï¸ Splitting large PDF ({pages} pages) for Parallel processing...")
        split_dir = Path(self.output_dir) / "temp_splits" / task_id
        chunks = split_pdf_file(Path(file_path), split_dir, chunk_size=chunk_size, parent_task_id=task_id)

        self.task_db.convert_to_parent_task(task_id, child_count=len(chunks))
        for chunk in chunks:
            # å­ä»»åŠ¡ç»§æ‰¿çˆ¶ä»»åŠ¡çš„æ‰€æœ‰é…ç½®
            self.task_db.create_child_task(
                parent_task_id=task_id,
                file_name=chunk["name"],
                file_path=chunk["path"],
                backend=task.get("backend", "pipeline"),
                options={**options, "chunk_info": chunk},
                priority=task.get("priority", 0),
                user_id=task.get("user_id")
            )
        return True

    def _process_with_mineru(self, file_path: str, options: dict) -> dict:
        """æœ¬åœ° GPU è§£æé€»è¾‘"""
        if not self.mineru_pipeline_engine:
            from mineru_pipeline import MinerUPipelineEngine
            self.mineru_pipeline_engine = MinerUPipelineEngine(device="cuda:0")
        
        output_dir = Path(self.output_dir) / Path(file_path).stem
        output_dir.mkdir(parents=True, exist_ok=True)
        res = self.mineru_pipeline_engine.parse(file_path, output_path=str(output_dir), options=options)
        normalize_output(Path(res["result_path"]))
        return res

    def _clean_memory(self):
        """æ˜¾å­˜é˜²æ³„æ¼æ¸…ç†"""
        try:
            import torch
            if torch.cuda.is_available(): torch.cuda.empty_cache()
            gc.collect()
        except: pass

    # LitServe æ¥å£å®ç°
    def decode_request(self, request): return request.get("action", "health")
    def predict(self, action): return {"status": "healthy", "worker": self.device}
    def encode_response(self, response): return response

# ============================================================================
# å¯åŠ¨å…¥å£
# ============================================================================
def start_litserve_workers(**kwargs):
    api = MinerUWorkerAPI(**kwargs)
    server = ls.LitServer(
        api, 
        accelerator=kwargs.get("accelerator", "auto"),
        devices=kwargs.get("devices", "auto"),
        workers_per_device=kwargs.get("workers_per_device", 1),
        timeout=False
    )
    server.run(port=kwargs.get("port", 8001))

if __name__ == "__main__":
    import argparse
    from utils import parse_list_arg
    
    parser = argparse.ArgumentParser()
    parser.add_argument("--port", type=int, default=8001)
    parser.add_argument("--devices", type=str, default="auto")
    parser.add_argument("--workers-per-device", type=int, default=1)
    parser.add_argument("--accelerator", type=str, default="cuda")
    parser.add_argument("--output-dir", type=str, default=None)
    
    args = parser.parse_args()
    start_litserve_workers(**vars(args))
