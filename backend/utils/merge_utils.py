"""
å¤©æ¢ä»»åŠ¡åˆå¹¶å·¥å…· (Production Version)
åŠŸèƒ½ï¼šå°† PDF åˆ†ç‰‡å¤„ç†åçš„å¤šä¸ªå­ä»»åŠ¡ç»“æœï¼ˆMarkdown/JSONï¼‰å®Œç¾æ— ç¼åˆå¹¶ã€‚
ç‰¹æ€§ï¼šæ”¯æŒé¡µç ä¿®æ­£ã€å…ƒæ•°æ®èšåˆã€å¼‚æ­¥ IO å®‰å…¨ã€‚
"""

import json
import os
import shutil
from pathlib import Path
from typing import List, Dict, Any
from loguru import logger

def merge_subtask_results(
    parent_task_id: str, 
    subtasks: List[Dict[str, Any]], 
    output_dir: Path
) -> Dict[str, Any]:
    """
    èšåˆæ‰€æœ‰å­ä»»åŠ¡çš„ç»“æœæ–‡ä»¶
    
    Args:
        parent_task_id: çˆ¶ä»»åŠ¡ ID
        subtasks: æ•°æ®åº“ä¸­æŸ¥å‡ºçš„å­ä»»åŠ¡åˆ—è¡¨ï¼ˆéœ€åŒ…å« result_path å’Œ optionsï¼‰
        output_dir: åˆå¹¶ç»“æœçš„å­˜å‚¨ç›®å½•
    """
    logger.info(f"ğŸ§© Starting merge for parent task: {parent_task_id} ({len(subtasks)} chunks)")
    
    # 1. æŒ‰ç…§èµ·å§‹é¡µç å¯¹å­ä»»åŠ¡è¿›è¡Œç‰©ç†æ’åº
    def get_start_page(task):
        try:
            # options å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–å­—å…¸ï¼Œå–å†³äºæ•°æ®åº“é©±åŠ¨è¿”å›ç±»å‹
            opts = task.get("options", {})
            if isinstance(opts, str):
                opts = json.loads(opts)
            return opts.get("chunk_info", {}).get("start_page", 0)
        except Exception:
            return 0

    sorted_tasks = sorted(subtasks, key=get_start_page)
    
    final_markdown = []
    final_json_pages = []
    total_images_copied = 0
    
    # åˆ›å»ºçˆ¶ä»»åŠ¡çš„æ­£å¼è¾“å‡ºç›®å½•
    parent_res_dir = output_dir / parent_task_id
    parent_res_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºç»Ÿä¸€çš„ images ç›®å½•
    final_image_dir = parent_res_dir / "images"
    final_image_dir.mkdir(exist_ok=True)

    # 2. é¡ºåºè¿­ä»£å¤„ç†æ¯ä¸ªåˆ†ç‰‡
    for task in sorted_tasks:
        if task["status"] != "completed" or not task.get("result_path"):
            continue
            
        chunk_path = Path(task["result_path"])
        if not chunk_path.exists():
            logger.warning(f"âš ï¸ Result path for subtask {task['task_id']} missing: {chunk_path}")
            continue

        # --- A. åˆå¹¶ Markdown ---
        # ä¼˜å…ˆæŸ¥æ‰¾ result.mdï¼Œå…¶æ¬¡æŸ¥æ‰¾ç›®å½•ä¸‹çš„ä»»æ„ .md
        md_file = next(chunk_path.rglob("result.md"), next(chunk_path.rglob("*.md"), None))
        if md_file:
            content = md_file.read_text(encoding="utf-8")
            # æ·»åŠ åˆ†ç‰‡æ³¨é‡Šï¼Œæ–¹ä¾¿æ’æŸ¥
            chunk_info = json.loads(task["options"]).get("chunk_info", {})
            marker = f"\n\n\n"
            final_markdown.append(marker + content)

        # --- B. åˆå¹¶å¹¶ä¿®æ­£ JSON ---
        json_file = next(chunk_path.rglob("result.json"), next(chunk_path.rglob("*_content_list.json"), None))
        if json_file:
            try:
                chunk_data = json.loads(json_file.read_text(encoding="utf-8"))
                # å¦‚æœæ˜¯ MinerU æ ¼å¼ï¼Œæ•°æ®åœ¨ 'pages' åˆ—è¡¨é‡Œ
                pages = chunk_data.get("pages", [])
                
                # è®¡ç®—é¡µç åç§»é‡
                # å¦‚æœåˆ†ç‰‡ 2 æ˜¯ä»ç¬¬ 51 é¡µå¼€å§‹ï¼Œoffset å°±æ˜¯ 50
                offset = get_start_page(task) - 1
                
                for page in pages:
                    if "page_idx" in page:
                        page["page_idx"] += offset
                    if "page_number" in page:
                        page["page_number"] += offset
                    final_json_pages.append(page)
            except Exception as e:
                logger.error(f"âŒ Failed to parse JSON for chunk {task['task_id']}: {e}")

        # --- C. è¿ç§»æœ¬åœ°å›¾ç‰‡ (å¦‚æœæœ‰) ---
        # æ³¨æ„ï¼šå¦‚æœå›¾ç‰‡å·²ä¸Šä¼  RustFSï¼ŒMarkdown é‡Œå·²ç»æ˜¯ URLï¼Œè¿™é‡Œåªéœ€è¿ç§»æœªä¸Šä¼ çš„æœ¬åœ°å¤‡ä»½
        chunk_image_dir = chunk_path / "images"
        if chunk_image_dir.exists():
            for img in chunk_image_dir.iterdir():
                if img.is_file():
                    shutil.copy2(img, final_image_dir / img.name)
                    total_images_copied += 1

    # 3. å†™å…¥æœ€ç»ˆæ–‡ä»¶
    final_md_path = parent_res_dir / "result.md"
    final_md_path.write_text("\n\n".join(final_markdown), encoding="utf-8")
    
    final_json_path = parent_res_dir / "result.json"
    with open(final_json_path, "w", encoding="utf-8") as f:
        json.dump({
            "parent_task_id": parent_task_id,
            "total_chunks": len(subtasks),
            "pages": final_json_pages
        }, f, ensure_ascii=False, indent=2)

    logger.success(f"âœ… Merge complete: {parent_task_id}")
    logger.info(f"   - Final Markdown: {final_md_path.stat().st_size / 1024:.1f} KB")
    logger.info(f"   - Final JSON: {len(final_json_pages)} pages")
    logger.info(f"   - Total Images: {total_images_copied}")

    return {
        "result_path": str(parent_res_dir),
        "markdown_path": str(final_md_path),
        "json_path": str(final_json_path)
    }
