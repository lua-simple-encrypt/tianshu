"""
å¤©æ¢ä»»åŠ¡åˆå¹¶å·¥å…· (Production Version)
åŠŸèƒ½ï¼šå°† PDF åˆ†ç‰‡å¤„ç†åçš„å¤šä¸ªå­ä»»åŠ¡ç»“æœï¼ˆMarkdown/JSONï¼‰å®Œç¾æ— ç¼åˆå¹¶ã€‚
ç‰¹æ€§ï¼šæ”¯æŒå…¨å±€é¡µç ä¿®æ­£ã€å›¾ç‰‡èµ„äº§èšåˆã€Options è‡ªåŠ¨è§£æã€‚
"""

import json
import os
import shutil
from pathlib import Path
from typing import List, Dict, Any, Optional
from loguru import logger

def merge_subtask_results(
    parent_task_id: str, 
    subtasks: List[Dict[str, Any]], 
    output_dir: Path
) -> Dict[str, Any]:
    """
    èšåˆæ‰€æœ‰å­ä»»åŠ¡çš„ç»“æœæ–‡ä»¶
    
    Args:
        parent_task_id: çˆ¶ä»»åŠ¡ ID
        subtasks: æ•°æ®åº“ä¸­æŸ¥å‡ºçš„å­ä»»åŠ¡åˆ—è¡¨ï¼ˆéœ€åŒ…å« result_path, options, statusï¼‰
        output_dir: åˆå¹¶ç»“æœçš„æ ¹å­˜å‚¨ç›®å½•
    """
    logger.info(f"ğŸ§© å¼€å§‹åˆå¹¶çˆ¶ä»»åŠ¡: {parent_task_id} (åˆ†ç‰‡æ•°: {len(subtasks)})")
    
    # 1. è§£æ Options å¹¶æŒ‰èµ·å§‹é¡µç å¯¹å­ä»»åŠ¡è¿›è¡Œç‰©ç†æ’åº
    def get_start_page(task):
        try:
            opts = task.get("options", {})
            # å…¼å®¹æ•°æ®åº“è¿”å›å­—ç¬¦ä¸²çš„æƒ…å†µ
            if isinstance(opts, str):
                opts = json.loads(opts)
            return opts.get("chunk_info", {}).get("start_page", 0)
        except Exception as e:
            logger.error(f"âŒ è§£æå­ä»»åŠ¡ Options å¤±è´¥: {e}")
            return 0

    # è¿‡æ»¤æ‰æœªå®Œæˆæˆ–æ— ç»“æœçš„åˆ†ç‰‡ï¼Œå¹¶æ’åº
    valid_subtasks = [t for t in subtasks if t.get("status") == "completed" and t.get("result_path")]
    sorted_tasks = sorted(valid_subtasks, key=get_start_page)
    
    if not sorted_tasks:
        raise ValueError(f"çˆ¶ä»»åŠ¡ {parent_task_id} ä¸‹æ²¡æœ‰å¯ç”¨çš„å·²å®Œæˆå­ä»»åŠ¡ç»“æœ")

    final_markdown = []
    final_json_pages = []
    total_images_copied = 0
    
    # åˆ›å»ºçˆ¶ä»»åŠ¡çš„æ­£å¼è¾“å‡ºç›®å½•
    parent_res_dir = output_dir / parent_task_id
    parent_res_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºç»Ÿä¸€çš„ images ç›®å½•
    final_image_dir = parent_res_dir / "images"
    final_image_dir.mkdir(exist_ok=True)

    # 2. é¡ºåºè¿­ä»£å¤„ç†æ¯ä¸ªåˆ†ç‰‡
    for task in sorted_tasks:
        chunk_path = Path(task["result_path"])
        if not chunk_path.exists():
            logger.warning(f"âš ï¸ å­ä»»åŠ¡ {task['task_id']} ç»“æœè·¯å¾„ä¸¢å¤±: {chunk_path}")
            continue

        # --- A. åˆå¹¶ Markdown ---
        # å¯»æ‰¾åˆ†ç‰‡å†…çš„ md æ–‡ä»¶
        md_file = next(chunk_path.rglob("result.md"), next(chunk_path.rglob("*.md"), None))
        if md_file:
            content = md_file.read_text(encoding="utf-8")
            # æ’å…¥åˆ†ç‰‡å ä½ç¬¦ï¼Œé˜²æ­¢æ®µè½ç²˜è¿
            start_pg = get_start_page(task)
            final_markdown.append(f"\n\n\n\n" + content)

        # --- B. åˆå¹¶å¹¶ä¿®æ­£ JSON (æ ¸å¿ƒéš¾ç‚¹) ---
        json_file = next(chunk_path.rglob("result.json"), next(chunk_path.rglob("*_content_list.json"), None))
        if json_file:
            try:
                chunk_data = json.loads(json_file.read_text(encoding="utf-8"))
                # æ”¯æŒ MinerU 2.x çš„ pages ç»“æ„
                pages = chunk_data.get("pages", [])
                
                # è®¡ç®—å…¨å±€åç§»é‡ (ä¾‹å¦‚ï¼šç¬¬äºŒåˆ†ç‰‡ä»51é¡µå¼€å§‹ï¼Œoffset=50)
                offset = get_start_page(task) - 1
                
                for page in pages:
                    # ä¿®æ­£é¡µç ç´¢å¼•
                    if "page_idx" in page:
                        page["page_idx"] += offset
                    if "page_number" in page:
                        page["page_number"] += offset
                    # ä¿®æ­£å±‚çº§ç»“æ„ä¸­çš„å­é¡µç å¼•ç”¨ï¼ˆå¦‚æœæœ‰ï¼‰
                    final_json_pages.append(page)
            except Exception as e:
                logger.error(f"âŒ ä¿®æ­£åˆ†ç‰‡ JSON ç´¢å¼•å¤±è´¥ {task['task_id']}: {e}")

        # --- C. è¿ç§»æœ¬åœ°å›¾ç‰‡èµ„äº§ ---
        # å­ä»»åŠ¡çš„å›¾ç‰‡ç›®å½•é€šå¸¸åœ¨ chunk_path/images/
        chunk_image_dir = chunk_path / "images"
        if chunk_image_dir.exists():
            for img in chunk_image_dir.iterdir():
                if img.is_file():
                    # è¿™é‡Œä½¿ç”¨ copy2 ä¿ç•™å…ƒæ•°æ®ï¼Œé˜²æ­¢é‡åç›´æ¥è¦†ç›–
                    target_img = final_image_dir / img.name
                    if not target_img.exists():
                        shutil.copy2(img, target_img)
                        total_images_copied += 1

    # 3. æŒä¹…åŒ–åˆå¹¶ç»“æœ
    final_md_path = parent_res_dir / "result.md"
    # ä½¿ç”¨ 3 ä¸ªæ¢è¡Œç¬¦ç¡®ä¿åˆ†ç‰‡é—´æœ‰æ¸…æ™°çš„è§†è§‰é—´éš”
    final_md_path.write_text("\n\n\n".join(final_markdown), encoding="utf-8")
    
    final_json_path = parent_res_dir / "result.json"
    with open(final_json_path, "w", encoding="utf-8") as f:
        json.dump({
            "parent_task_id": parent_task_id,
            "total_pages": len(final_json_pages),
            "merged_chunks": len(sorted_tasks),
            "pages": final_json_pages
        }, f, ensure_ascii=False, indent=2)

    logger.success(f"âœ… ä»»åŠ¡åˆå¹¶å®Œæˆ: {parent_task_id}")
    logger.info(f"   - Markdown å¤§å°: {final_md_path.stat().st_size / 1024:.1f} KB")
    logger.info(f"   - JSON é¡µæ•°: {len(final_json_pages)}")
    logger.info(f"   - èšåˆå›¾ç‰‡æ•°: {total_images_copied}")

    return {
        "result_path": str(parent_res_dir),
        "markdown_path": str(final_md_path),
        "json_path": str(final_json_path)
    }
